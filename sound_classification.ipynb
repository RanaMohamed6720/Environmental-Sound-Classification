{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "678ae028",
   "metadata": {},
   "source": [
    "# Environmental Sound Classification (UrbanSound8K dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e97584c",
   "metadata": {},
   "source": [
    "## Goal\n",
    "Build RNN and Transformer model that can identify 10 types of urban sounds from UrbanSound8K dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711cf06f",
   "metadata": {},
   "source": [
    "## 0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45b49ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundata\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc79b41",
   "metadata": {},
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676d04cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self,config):\n",
    "        self.config = config\n",
    "        self.dataset = None\n",
    "        self.clips = None \n",
    "        self.feature_data = {\n",
    "            'train': {'features':[],'labels':[],'folds':[]},\n",
    "            'val':{'features':[],'labels':[],'folds':[]},\n",
    "            'test':{'features':[],'labels':[],'folds':[]}\n",
    "        }\n",
    "\n",
    "    def download_and_validate(self):\n",
    "        print(\"Initializing dataset\")\n",
    "        self.dataset = soundata.initialize('urbansound8k')\n",
    "\n",
    "        try:\n",
    "            # in case it is already downloaded\n",
    "            print(\"Validating dataset\")\n",
    "            self.dataset.validate()\n",
    "        except Exception as e:\n",
    "            print(\"Downloading dataset\")\n",
    "            self.dataset.download()\n",
    "\n",
    "        self.clips = self.dataset.load_clips()\n",
    "        print(f\"Loaded {len(self.clips)} audio clips\")\n",
    "\n",
    "    def get_fold_number(self,clip_id):\n",
    "        clip = self.clips[clip_id]\n",
    "        if hasattr(clip, '_clip_metadata'):\n",
    "            metadata = clip._clip_metadata\n",
    "            if isinstance(metadata, dict) and 'fold' in metadata:\n",
    "                return int(metadata['fold'])\n",
    "                \n",
    "    def preprocess_audio(self, audio, original):\n",
    "        # 1. resample audio to standardize inputs ensuring all audio files have same sample rate for consistent processing\n",
    "        if original != self.config['sample_rate']:\n",
    "            # initialize pytorch resampler with original and target sample rates\n",
    "            resampler = torchaudio.transforms.Resample(\n",
    "                orig_freq=original,                     # original sample rate\n",
    "                new_freq=self.config['sample_rate']     # target sample rate\n",
    "            )\n",
    "\n",
    "            audio = resampler(torch.tensor(audio).float()).numpy()\n",
    "        \n",
    "        # 2. channel normalization \n",
    "        # converting to mono if multi channel so it could train faster and it is better for sound recognition\n",
    "        if audio.ndim > 1:\n",
    "            audio = librosa.to_mono(audio)\n",
    "        # 3. duration standarization\n",
    "        # target length = duration * sample rate\n",
    "        target_samples = int(self.config['max_duration'] * self.config['sample_rate'])\n",
    "        \n",
    "        # case1: audio is shorter than target duration\n",
    "        if len(audio) < target_samples:\n",
    "            # calculate padding needed (only at end of audio)\n",
    "            pad_amount = target_samples - len(audio)\n",
    "            # add zero padding (silence) at the end\n",
    "            audio = np.pad(audio, (0, pad_amount))\n",
    "        \n",
    "        # case2: audio is longer than target duration\n",
    "        else:\n",
    "            # truncate to the target length (keep beginning portion)\n",
    "            audio = audio[:target_samples]\n",
    "        return audio\n",
    "    \n",
    "    def extract_features(self,audio):\n",
    "        # mel spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=audio,  \n",
    "            sr=self.config['sample_rate'], \n",
    "            n_fft=self.config['n_fft'],  \n",
    "            hop_length=self.config['hop_length'],  \n",
    "            n_mels=self.config['n_mels']  \n",
    "        )\n",
    "        # convert power spectrogram to dB scale (log transform) which matches human hearing's perception\n",
    "        log_mel = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "        # RMS -> per frame loudness estimate\n",
    "        energy = librosa.feature.rms(\n",
    "            y=audio,\n",
    "            frame_length=self.config['n_fft'],  \n",
    "            hop_length=self.config['hop_length']\n",
    "        )\n",
    "\n",
    "        # MFCCs -> Mel Frequency Cepstral Coefficients\n",
    "        # compact spectral representation\n",
    "        mfcc = librosa.feature.mfcc(\n",
    "            y=audio,\n",
    "            sr=self.config['sample_rate'],\n",
    "            n_mfcc=self.config['n_mfcc'],  \n",
    "            n_fft=self.config['n_fft'],\n",
    "            hop_length=self.config['hop_length']\n",
    "        )\n",
    "        return {\n",
    "            'waveform': audio,  \n",
    "            'log_mel': log_mel,\n",
    "            'energy': energy,  \n",
    "            'mfcc': mfcc  \n",
    "        }\n",
    "    def process_dataset(self):\n",
    "        print(\"Processing dataset\")\n",
    "        for clip_id, clip in tqdm(self.clips.items()):\n",
    "            audio, sr = clip.audio\n",
    "            fold = self.get_fold_number(clip_id)\n",
    "            class_name = clip.tags.labels[0]\n",
    "            \n",
    "            audio = self.preprocess_audio(audio, sr)\n",
    "            features = self.extract_features(audio)\n",
    "            \n",
    "            split = self.get_split(fold)\n",
    "            if split:\n",
    "                self.feature_data[split]['features'].append(features)\n",
    "                self.feature_data[split]['labels'].append(class_name)\n",
    "                self.feature_data[split]['folds'].append(fold)\n",
    "\n",
    "    def get_split(self, fold):\n",
    "        if fold in self.config['train_folds']:\n",
    "            return 'train'\n",
    "        elif fold in self.config['val_folds']:\n",
    "            return 'val'\n",
    "        elif fold in self.config['test_folds']:\n",
    "            return 'test'\n",
    "        return None\n",
    "    \n",
    "    def get_processed_data(self):\n",
    "        return {\n",
    "            'train': {\n",
    "                'features': np.array(self.feature_data['train']['features']),\n",
    "                'labels': np.array(self.feature_data['train']['labels']),\n",
    "                'folds': np.array(self.feature_data['train']['folds'])\n",
    "            },\n",
    "            'val': {\n",
    "                'features': np.array(self.feature_data['val']['features']),\n",
    "                'labels': np.array(self.feature_data['val']['labels']),\n",
    "                'folds': np.array(self.feature_data['val']['folds'])\n",
    "            },\n",
    "            'test': {\n",
    "                'features': np.array(self.feature_data['test']['features']),\n",
    "                'labels': np.array(self.feature_data['test']['labels']),\n",
    "                'folds': np.array(self.feature_data['test']['folds'])\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91410f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing dataset\n",
      "Validating dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 335.89it/s]\n",
      "100%|██████████| 8732/8732 [00:28<00:00, 309.09it/s]\n",
      "INFO: Success: the dataset is complete and all files are valid.\n",
      "INFO: --------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 8732 audio clips\n",
      "Processing dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8732/8732 [05:34<00:00, 26.11it/s]\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'sample_rate': 16000,\n",
    "    'max_duration': 4.0,\n",
    "    'n_fft': 2048,\n",
    "    'hop_length': 512,\n",
    "    'n_mels': 64,\n",
    "    'n_mfcc': 20,\n",
    "    'train_folds': list(range(1, 7)),\n",
    "    'val_folds': [7, 8],\n",
    "    'test_folds': [9, 10],\n",
    "    'resample': True\n",
    "}\n",
    "\n",
    "data_loader = DataLoader(config)\n",
    "data_loader.download_and_validate()\n",
    "data_loader.process_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "727a7093",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = data_loader.get_processed_data()\n",
    "\n",
    "# training data\n",
    "train_features = processed_data['train']['features']\n",
    "train_labels = processed_data['train']['labels']\n",
    "train_folds = processed_data['train']['folds']\n",
    "\n",
    "# validation data\n",
    "val_features = processed_data['val']['features']\n",
    "val_labels = processed_data['val']['labels']\n",
    "val_folds = processed_data['val']['folds']\n",
    "\n",
    "# test data\n",
    "test_features = processed_data['test']['features']\n",
    "test_labels = processed_data['test']['labels']\n",
    "test_folds = processed_data['test']['folds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02420930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset statistics:\n",
      "Training set: 5435 samples\n",
      "Validation set: 1644 samples\n",
      "Test set: 1653 samples\n",
      "\n",
      "Feature Shapes (first sample):\n",
      "Waveform: (64000,)\n",
      "Log-Mel: (64, 126)\n",
      "MFCCs: (20, 126)\n",
      "Energy: (1, 126)\n",
      "\n",
      "Class distribution:\n",
      "Training classes: ['air_conditioner' 'car_horn' 'children_playing' 'dog_bark' 'drilling'\n",
      " 'engine_idling' 'gun_shot' 'jackhammer' 'siren' 'street_music']\n",
      "Validation classes: ['air_conditioner' 'car_horn' 'children_playing' 'dog_bark' 'drilling'\n",
      " 'engine_idling' 'gun_shot' 'jackhammer' 'siren' 'street_music']\n",
      "Test classes: ['air_conditioner' 'car_horn' 'children_playing' 'dog_bark' 'drilling'\n",
      " 'engine_idling' 'gun_shot' 'jackhammer' 'siren' 'street_music']\n",
      "\n",
      "Fold distribution:\n",
      "Training folds: [1 2 3 4 5 6]\n",
      "Validation folds: [7 8]\n",
      "Test folds: [ 9 10]\n"
     ]
    }
   ],
   "source": [
    "# dataset statistics\n",
    "print(\"\\nDataset statistics:\")\n",
    "print(f\"Training set: {len(train_features)} samples\")\n",
    "print(f\"Validation set: {len(val_features)} samples\")\n",
    "print(f\"Test set: {len(test_features)} samples\")\n",
    "\n",
    "# sample shapes\n",
    "print(\"\\nFeature Shapes (first sample):\")\n",
    "print(f\"Waveform: {train_features[0]['waveform'].shape}\")\n",
    "print(f\"Log Mel: {train_features[0]['log_mel'].shape}\")\n",
    "print(f\"MFCCs: {train_features[0]['mfcc'].shape}\")\n",
    "print(f\"Energy: {train_features[0]['energy'].shape}\")\n",
    "\n",
    "# class distribution in each set \n",
    "print(\"\\nClass distribution:\")\n",
    "print(f\"Training classes: {np.unique(train_labels)}\")\n",
    "print(f\"Validation classes: {np.unique(val_labels)}\")\n",
    "print(f\"Test classes: {np.unique(test_labels)}\")\n",
    "\n",
    "# fold distribution\n",
    "print(\"\\nFold distribution:\")\n",
    "print(f\"Training folds: {np.unique(train_folds)}\")\n",
    "print(f\"Validation folds: {np.unique(val_folds)}\")\n",
    "print(f\"Test folds: {np.unique(test_folds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc341be",
   "metadata": {},
   "source": [
    "## 2. Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617c56ed",
   "metadata": {},
   "source": [
    "### 2.1 RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4d2a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1a3438f",
   "metadata": {},
   "source": [
    "### 2.2 Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151acf60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c224f240",
   "metadata": {},
   "source": [
    "## 3. Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a825ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
